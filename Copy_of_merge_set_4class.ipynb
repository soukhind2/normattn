{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of merge_set_4class.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "La-h_mR0ygZN",
        "2Gi97SdGyrE2",
        "KmnNRm50Q05-",
        "jJIu_3X-iLeP",
        "OiF5A5hzMSjT",
        "yoC7ZmIr95_E",
        "et83rH-l93zU",
        "-etj3D0SVwfb",
        "fP0gLmlmVz7q",
        "wY8oDGHaSuqf",
        "d-Pl1kB3Q3vO",
        "9C8vZVeCQ9Dj"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soukhind2/normattn/blob/beta/Copy_of_merge_set_4class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uhBNXwh_w9B"
      },
      "source": [
        "# Preload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNH3vmYgxdao"
      },
      "source": [
        "Mount the google drive and load necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "tWeNtkRpYWgy",
        "outputId": "10e31688-a85c-4b28-c326-cbe9d0e44bd4"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b637752fdce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    290\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paH-Z9zPYk6t"
      },
      "source": [
        "!rm -rf vgg16obj\n",
        "!git clone -b tuning_calc https://github.com/soukhind2/vgg16obj\n",
        "! pip install https://github.com/raghakot/keras-vis/archive/master.zip #keras vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVSWxf5qxoSv"
      },
      "source": [
        "Unpack the items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL5079ZkYpf5"
      },
      "source": [
        "#!unzip -q /content/drive/My\\ Drive/obj/data\n",
        "#!unzip -q /content/drive/My\\ Drive/obj/test2\n",
        "#!unzip -q /content/drive/My\\ Drive/obj/sean_test\n",
        "!unzip -q /content/drive/My\\ Drive/obj/data_6class\n",
        "!unzip -q /content/drive/My\\ Drive/obj/merge\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxfdgi8hHv_1"
      },
      "source": [
        "# Load libraries\n",
        "Load the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdIPynMYsJj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "\n",
        "from keras.applications.vgg16 import VGG16,preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.layers import Flatten,Dense,Dropout,Input,BatchNormalization\n",
        "from keras.models import Sequential,Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from vgg16obj.tools import tuning_calcs as tc\n",
        "from vgg16obj.tools import gradient_calcs as gc\n",
        "from vgg16obj.tools import stats as st\n",
        "from vgg16obj.tools import plot_tools as pt\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPKnbq8zZAEI"
      },
      "source": [
        "# Load Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3AFr11WZBql"
      },
      "source": [
        "### Merged Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWkKJ-FAY8Dq"
      },
      "source": [
        "def noisy(image):\n",
        "  row,col,ch= image.shape\n",
        "  mean = 0\n",
        "  var = 1\n",
        "  sigma = var**0.5\n",
        "  gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
        "  gauss = gauss.reshape(row,col,ch)\n",
        "  noisy = image + gauss\n",
        "  return noisy\n",
        "\n",
        "def convertimgs(path,noise = False) :\n",
        "    data = []\n",
        "    for dirName, subdir, files in os.walk(path):\n",
        "        for filename in sorted(files):\n",
        "            if filename == '.DS_Store':\n",
        "                continue\n",
        "            ds = load_img(path +'/' + filename,target_size = (224,224))\n",
        "            im = img_to_array(ds)\n",
        "            im /= 255.\n",
        "            #im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "            #im = preprocess_input(im)\n",
        "            if noise:\n",
        "              im = noisy(im)\n",
        "            data.append(im) \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P267h5kYZE_H"
      },
      "source": [
        "# Merged images load\n",
        "data_train = [[]  for i in range(8)]\n",
        "data_train[0] = convertimgs('/content/merge/merge_train/Correct/Male',noise = False)  #75\n",
        "data_train[1] = convertimgs('/content/merge/merge_train/Correct/Female/',noise = False) #75\n",
        "data_train[2] = convertimgs('/content/merge/merge_train/Correct/Manmade/',noise = False) #75\n",
        "data_train[3] = convertimgs('/content/merge/merge_train/Correct/Natural/',noise = False) #75\n",
        "#data_train[4] = convertimgs('/content/merge/merge_train/Correct/Powered',noise = False) #75\n",
        "#data_train[5] = convertimgs('/content/merge/merge_train/Correct/Nonpowered',noise = False) #75\n",
        "\n",
        "data_train[4] = convertimgs('/content/merge/merge_train/Incorrect/Male',noise = False) #75\n",
        "data_train[5] = convertimgs('/content/merge/merge_train/Incorrect/Female',noise = False) #75\n",
        "data_train[6] = convertimgs('/content/merge/merge_train/Incorrect/Manmade',noise = False) #75\n",
        "data_train[7] = convertimgs('/content/merge/merge_train/Incorrect/Natural',noise = False) #75\n",
        "#data_train[10] = convertimgs('/content/merge/merge_train/Incorrect/Powered',noise = False) #75\n",
        "#data_train[11] = convertimgs('/content/merge/merge_train/Incorrect/Nonpowered',noise = False) #75\n",
        "data_train = np.array(data_train)\n",
        " \n",
        "data_test = [[]  for i in range(8)]\n",
        "data_test[0] = convertimgs('/content/merge/merge_test/Correct/Male',noise = False)  #75\n",
        "data_test[1] = convertimgs('/content/merge/merge_test/Correct/Female/',noise = False) #75\n",
        "data_test[2] = convertimgs('/content/merge/merge_test/Correct/Manmade/',noise = False) #75\n",
        "data_test[3] = convertimgs('/content/merge/merge_test/Correct/Natural/',noise = False) #75\n",
        "#data_test[4] = convertimgs('/content/merge/merge_test/Correct/Powered',noise = False) #75\n",
        "#data_test[5] = convertimgs('/content/merge/merge_test/Correct/Nonpowered',noise = False) #75\n",
        "\n",
        "data_test[4] = convertimgs('/content/merge/merge_test/Incorrect/Male',noise = False) #75\n",
        "data_test[5] = convertimgs('/content/merge/merge_test/Incorrect/Female',noise = False) #75\n",
        "data_test[6] = convertimgs('/content/merge/merge_test/Incorrect/Manmade',noise = False) #75\n",
        "data_test[7] = convertimgs('/content/merge/merge_test/Incorrect/Natural',noise = False) #75\n",
        "#data_test[10] = convertimgs('/content/merge/merge_test/Incorrect/Powered',noise = False) #75\n",
        "#data_test[11] = convertimgs('/content/merge/merge_test/Incorrect/Nonpowered',noise = False) #75\n",
        "data_test = np.array(data_test)\n",
        "\n",
        "\n",
        "print(data_train.shape,data_test.shape)\n",
        "\n",
        "plt.imshow(data_train[3,79])\n",
        "plt.axis('off')\n",
        "\n",
        "# Regular Images load\n",
        "\n",
        "reg_train = [[]  for i in range(8)]\n",
        "reg_train[0] = convertimgs('/content/merge/merge_reg_train/Correct/Male',noise = False)  # 75\n",
        "reg_train[1] = convertimgs('/content/merge/merge_reg_train/Correct/Female',noise = False) # 75\n",
        "reg_train[2] = convertimgs('/content/merge/merge_reg_train/Correct/Manmade',noise = False) # 75\n",
        "reg_train[3] = convertimgs('/content/merge/merge_reg_train/Correct/Natural/',noise = False) # 75\n",
        "#reg_train[4] = convertimgs('/content/merge/merge_reg_train/Correct/Powered',noise = False) # 75\n",
        "#reg_train[5] = convertimgs('/content/merge/merge_reg_train/Correct/Nonpowered',noise = False) # 75\n",
        "\n",
        "reg_train[4] = convertimgs('/content/merge/merge_reg_train/Incorrect/Male',noise = False)  # 75\n",
        "reg_train[5] = convertimgs('/content/merge/merge_reg_train/Incorrect/Female',noise = False) # 75\n",
        "reg_train[6] = convertimgs('/content/merge/merge_reg_train/Incorrect/Manmade',noise = False) # 75\n",
        "reg_train[7] = convertimgs('/content/merge/merge_reg_train/Incorrect/Natural/',noise = False) # 75\n",
        "#reg_train[10] = convertimgs('/content/merge/merge_reg_train/Incorrect/Powered',noise = False) # 75\n",
        "#reg_train[11] = convertimgs('/content/merge/merge_reg_train/Incorrect/Nonpowered',noise = False) # 75\n",
        "reg_train = np.array(reg_train)\n",
        "\n",
        "\n",
        "reg_test = [[]  for i in range(8)]\n",
        "reg_test[0] = convertimgs('/content/merge/merge_reg_test/Correct/Male',noise = False)  #75\n",
        "reg_test[1] = convertimgs('/content/merge/merge_reg_test/Correct/Female/',noise = False) #75\n",
        "reg_test[2] = convertimgs('/content/merge/merge_reg_test/Correct/Manmade/',noise = False) #75\n",
        "reg_test[3] = convertimgs('/content/merge/merge_reg_test/Correct/Natural/',noise = False) #75\n",
        "#reg_test[4] = convertimgs('/content/merge/merge_reg_test/Correct/Powered',noise = False) #75\n",
        "#reg_test[5] = convertimgs('/content/merge/merge_reg_test/Correct/Nonpowered',noise = False) #75\n",
        "\n",
        "reg_test[4] = convertimgs('/content/merge/merge_reg_test/Incorrect/Male',noise = False) #75\n",
        "reg_test[5] = convertimgs('/content/merge/merge_reg_test/Incorrect/Female',noise = False) #75\n",
        "reg_test[6] = convertimgs('/content/merge/merge_reg_test/Incorrect/Manmade',noise = False) #75\n",
        "reg_test[7] = convertimgs('/content/merge/merge_reg_test/Incorrect/Natural',noise = False) #75\n",
        "#reg_test[10] = convertimgs('/content/merge/merge_reg_test/Incorrect/Powered',noise = False) #75\n",
        "#reg_test[11] = convertimgs('/content/merge/merge_reg_test/Incorrect/Nonpowered',noise = False) #75\n",
        "reg_test = np.array(reg_test)\n",
        "\n",
        "print(reg_train.shape,reg_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJqRd2ccJXG"
      },
      "source": [
        "# Model\n",
        "Call the base model and the top model here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QLK6addybWJ"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsm9BSiwGctD"
      },
      "source": [
        "# data preprocessing:\n",
        "\n",
        "categories = ['Male','Female','Manmade','Natural']\n",
        "interest = 0\n",
        "train_it = np.concatenate((reg_train[interest],reg_train[interest + 6]))\n",
        "test_it = np.concatenate((reg_test[interest],reg_test[interest + 6]))\n",
        "\n",
        "#================================================================================================================================\n",
        "#alexnet:\n",
        "\n",
        "#bottom layers:\n",
        "alexnet = tf.keras.Sequential()\n",
        "\n",
        "#input image size= 224x224x3\n",
        "\n",
        "# conv1\n",
        "alexnet.add(tf.keras.layers.Conv2D(filters = 96, kernel_size = (11,11), input_shape= (55, 55, 48), activation='relu', strides=(4,4), padding='same')) \n",
        "alexnet.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "\n",
        "# conv2\n",
        "alexnet.add(tf.keras.layers.Conv2D(filters = 256, kernel_size = (5,5), input_shape=(27, 27, 128), activation='relu', strides=(1,1), padding='same'))\n",
        "alexnet.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#conv3\n",
        "alexnet.add(tf.keras.layers.Conv2D(filters = 384, kernel_size = (3,3), input_shape=(13, 13, 192), activation='relu',strides=(1,1), padding='same'))\n",
        "\n",
        "#conv4\n",
        "alexnet.add(tf.keras.layers.Conv2D(filters = 384, kernel_size = (3,3), input_shape=(13, 13, 192), activation='relu',strides=(1,1), padding='same'))\n",
        "\n",
        "#conv5\n",
        "alexnet.add(tf.keras.layers.Conv2D(filters = 256, kernel_size = (3,3), input_shape=(13, 13, 128),activation='relu',strides=(1,1), padding='same'))\n",
        "alexnet.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "# alexnet.summary()\n",
        "\n",
        "alexnet.load_weights(\"/content/alexnet.weights\")\n",
        "\n",
        "\n",
        "features_train = alexnet.predict(train_it) \n",
        "features_test = alexnet.predict(test_it) \n",
        "\n",
        "# top layers:\n",
        "#FC1\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=features_train.shape[1:])) \n",
        "\n",
        "#FC2\n",
        "top_model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
        "\n",
        "#FC3\n",
        "top_model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
        "\n",
        "# referenced this paper: https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
        "\n",
        "#others:\n",
        "epochs = 30\n",
        "ntrain = 80\n",
        "train_labels = to_categorical([0] * ntrain + [1]*ntrain)\n",
        "ntest = 40\n",
        "test_labels = to_categorical([0] * ntest + [1]*ntest)\n",
        "losses = 'binary_crossentropy'\n",
        "\n",
        "top_model.compile(optimizer= Adam(lr=1e-5),\n",
        "              loss=losses,\n",
        "              metrics=['accuracy'])\n",
        "top_model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkVYlVBT3Kbu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La-h_mR0ygZN"
      },
      "source": [
        "### Model Testing\n",
        "\n",
        "Train the model on regular images, and then test separately on regular images\n",
        "followed by testing separately for merged images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBvaXyRraKrN"
      },
      "source": [
        "acc = np.zeros((2,4))\n",
        "for imtype in range(2):\n",
        "  for cat in range(4):\n",
        "    print('Category of interest: ', categories[cat])\n",
        "    train_it = np.concatenate((reg_train[cat],reg_train[cat + 4])) # Train on regular data\n",
        "    if imtype == 0: # Regular\n",
        "      test_it = np.concatenate((reg_test[cat],reg_test[cat + 4])) # Test on merged data\n",
        "    else:\n",
        "      test_it = np.concatenate((data_test[cat],data_test[cat + 4])) # Test on merged data\n",
        "    print(train_it.shape,test_it.shape)\n",
        "\n",
        "    \n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=features_train.shape[1:])) \n",
        "    top_model.add(Dense(4096, activation='relu',name = 'top_dense1')) \n",
        "    top_model.add(Dense(2, activation='softmax',name = 'predictions'))\n",
        "\n",
        "    top_model.compile(optimizer= Adam(lr=1e-5),\n",
        "                  loss=losses,\n",
        "                  metrics=['accuracy'])\n",
        "    start = time.time()\n",
        "    train_data = model.predict(train_it) \n",
        "    print(f'Train Time: {time.time() - start}')\n",
        "\n",
        "    start = time.time()\n",
        "    test_data = model.predict(test_it) \n",
        "    print(f'Test Time: {time.time() - start}')\n",
        "    \n",
        "    history = top_model.fit(x = train_data,  y = train_labels,\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            verbose = 1, callbacks = [es])\n",
        "\n",
        "    out = top_model.evaluate(test_data, test_labels)\n",
        "    acc[imtype,cat] = out[1]\n",
        "    print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gi97SdGyrE2"
      },
      "source": [
        "### Prelim result plots\n",
        "Plot the average accuracy obtained for regular vs merged images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvIZeqnRgQ4j"
      },
      "source": [
        "diff = acc[0]-acc[1]\n",
        "print(np.mean(acc))\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "sns.color_palette('pastel')\n",
        "sns.set(font_scale=2,style=\"ticks\")\n",
        "plt.figure(figsize = (4,8),tight_layout = True)\n",
        "ax = sns.boxplot(data = acc.T*100 )\n",
        "#ax.set_xlabel('Category',size = 20)\n",
        "ax.set_xticklabels(['Regular','Merged'],size = 20)\n",
        "ax.set_ylim(50,100)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "#ax.set_ylabel('Binary Classification Acc.',size = 20)\n",
        "#ax.set_yticks([0.6,0.7,0.8,0.9,1])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRcNO6SiiCOd"
      },
      "source": [
        "# Tuning Value Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3fPlYt_uupE"
      },
      "source": [
        "calc_tun_activ = 1\n",
        "\n",
        "if calc_tun_activ:\n",
        "  for cat in range(len(categories)): \n",
        "\n",
        "    print('Category of interest: ', categories[cat])\n",
        "    train_it = reg_train[cat]\n",
        "\n",
        "    layer_names = [layer.name for layer in model.layers if 'conv' in layer.name]\n",
        "    layer_outputs = [layer.output for layer in model.layers if layer.name in layer_names]\n",
        "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "\n",
        "    start = time.time()\n",
        "    tun_activ = tc.calc_tun(train_it,activation_model,layer_names)\n",
        "    print(f'Elapsed: {time.time() - start}s')\n",
        "\n",
        "    with open('tuning_values_' + str(cat), 'wb') as fp:\n",
        "      pickle.dump(tun_activ, fp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmnNRm50Q05-"
      },
      "source": [
        "### Load tuning activities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9i3JjziHTN"
      },
      "source": [
        "tun_activ = []\n",
        "for interest in range(len(categories)): \n",
        "  with open ('tuning_values_' + str(interest), 'rb') as fp:\n",
        "      tun_activ.extend(pickle.load(fp))\n",
        "len(tun_activ)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJIu_3X-iLeP"
      },
      "source": [
        "### Calculation of tuning values for each map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6C47jSjiJXP"
      },
      "source": [
        "ncats = len(categories)\n",
        "# to fish out each category tun_activations\n",
        "labels = np.array([0] * 80 + [1] * 80 + [2] * 80 + [3] * 80 )\n",
        "cat_tun = [[[] for j in range(len(tun_activ[0]))] for i in range(ncats)]\n",
        "\n",
        "avg_tun_activ = tc.calc_avg(tun_activ) #average tuning activity for each map\n",
        "std_tun_activ = tc.calc_std(tun_activ,avg_tun_activ) #std tuning activity for each map\n",
        "\n",
        "for i in range(ncats):\n",
        "  #Calculating average activity of each\n",
        "  #feature map in response to images of respective category, \n",
        "  #with the mean activity under all image categories subtracted from it\n",
        "  idx = list(np.where(labels == i))\n",
        "  cat_tun[i] = tc.calc_avg([tun_activ[i] for i in idx[0]])\n",
        "\n",
        "# Vector of tuning values for each obj cataegory\n",
        "fc = [[[0 for item in subl] for subl in cat_tun[0]] for i in range(ncats)]\n",
        "#fc has length of ncats x nlayers x no of maps in each layer\n",
        "for cat in range(ncats):\n",
        "  for layer in range(len(cat_tun[cat])):\n",
        "    for map in range(len(cat_tun[cat][layer])):\n",
        "      if std_tun_activ[layer][map] == 0:\n",
        "        continue\n",
        "      fc[cat][layer][map] = (cat_tun[cat][layer][map] - \n",
        "                                avg_tun_activ[layer][map])/std_tun_activ[layer][map]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiF5A5hzMSjT"
      },
      "source": [
        "### Tuning quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOSl0b-UiRnH"
      },
      "source": [
        "tun_quality = tc.calc_tun_quality(fc)\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = tun_quality,palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoC7ZmIr95_E"
      },
      "source": [
        "### Tuning quality for face images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcxFzTBfK-PU"
      },
      "source": [
        "tun_q_face = [[0 for item in subl] for subl in fc[0]]\n",
        "for cat in range(2):\n",
        "  for layer in range(len(fc[cat])):\n",
        "    for map in range(len(fc[cat][layer])):\n",
        "      if tun_q_face[layer][map] < fc[cat][layer][map]:\n",
        "        tun_q_face[layer][map] = fc[cat][layer][map]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = tun_q_face,palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,50])\n",
        "plt.title('Tuning quality for Face Images')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et83rH-l93zU"
      },
      "source": [
        "### Tuning quality for non face neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iafMwylj93N4"
      },
      "source": [
        "tun_q_face = [[0 for item in subl] for subl in fc[0]]\n",
        "for cat in range(2,4):\n",
        "  for layer in range(len(fc[cat])):\n",
        "    for map in range(len(fc[cat][layer])):\n",
        "      if tun_q_face[layer][map] < fc[cat][layer][map]:\n",
        "        tun_q_face[layer][map] = fc[cat][layer][map]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = tun_q_face,palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,50])\n",
        "plt.title('Tuning quality for Scene Images')\n",
        "\n",
        "plt.figure()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtKxd454A1_N"
      },
      "source": [
        "idx = [[-1 for item in subl] for subl in fc[0]] \n",
        "for layer in range(13):\n",
        "  for map in range(len(fc[0][layer])):\n",
        "    max = 0\n",
        "    for cat in range(ncats):\n",
        "      if fc[cat][layer][map] > max:\n",
        "        max = fc[cat][layer][map]\n",
        "        idx[layer][map] = cat\n",
        "\n",
        "counter = np.zeros(13)\n",
        "fig = plt.figure(figsize = (10,8))\n",
        "for layer in range(13):\n",
        "  for ele in idx[layer]:\n",
        "    if ele == 0 or ele == 1:\n",
        "      counter[layer] += 1\n",
        "plt.bar(x = np.arange(1,14,1),\n",
        "        height = counter,\n",
        "        color = 'mediumseagreen')\n",
        "plt.ylim(0,500)\n",
        "plt.xlabel('Layer',size = 15)\n",
        "plt.ylabel('No. of neurons',size = 15)\n",
        "plt.title('Number of face selective neurons',size = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHE21BdChtO"
      },
      "source": [
        "width = 0.25\n",
        "\n",
        "idx = [[-1 for item in subl] for subl in fc[0]] \n",
        "for layer in range(13):\n",
        "  for map in range(len(fc[0][layer])):\n",
        "    max = 0\n",
        "    for cat in range(ncats):\n",
        "      if fc[cat][layer][map] > max:\n",
        "        max = fc[cat][layer][map]\n",
        "        idx[layer][map] = cat\n",
        "\n",
        "c_face = np.zeros(13)\n",
        "c_scene = np.zeros(13)\n",
        "\n",
        "\n",
        "for layer in range(13):\n",
        "  for ele in idx[layer]:\n",
        "    if ele == 0 or ele == 1:\n",
        "      c_face[layer] += 1\n",
        "    elif ele == 2 or ele ==3:\n",
        "      c_scene[layer] += 1\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10,8))\n",
        "rects1 = ax.bar(np.arange(13) - width/2 , c_face, width, label='Faces',color = '#D65353')\n",
        "rects2 = ax.bar(np.arange(13) + width/2, c_scene, width, label='Scenes',color = '#F7C45F')\n",
        "\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Scores by group and gender')\n",
        "ax.set_xticks(np.arange(13))\n",
        "ax.set_xticklabels(np.arange(13))\n",
        "ax.legend()\n",
        "\n",
        "plt.ylim(0,500)\n",
        "plt.xticks(np.arange(13),np.arange(1,14),size = 12)\n",
        "plt.xlabel('Layer',size = 15)\n",
        "plt.ylabel('No. of Neurons',size = 15)\n",
        "plt.title('Number of selective neurons, category wise', size = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4cG1djAJoda"
      },
      "source": [
        "#Gradient Calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1DAyeXoJmIB"
      },
      "source": [
        "model = VGG16(weights='imagenet',\n",
        "              include_top=False,input_shape = [224,224,3])\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=features_train.shape[1:]))\n",
        "top_model.add(Dense(4096, activation='relu',name = 'top_dense1'))\n",
        "top_model.add(Dense(4, activation='softmax',name = 'predictions'))\n",
        "top_model.compile(optimizer= Adam(lr=1e-5),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "g_labels_train = to_categorical(labels)\n",
        "g_labels_test = to_categorical([0]* 40 + [1]*40 + [2]*40 + [3]*40)\n",
        "g_train = np.vstack((reg_train[0],reg_train[1],reg_train[2],reg_train[3]))\n",
        "g_test = np.vstack((reg_test[0],reg_test[1],reg_test[2],reg_test[3]))\n",
        "\n",
        "G_train = model.predict(g_train)\n",
        "G_test = model.predict(g_test)\n",
        "\n",
        "history = top_model.fit(x = G_train,  y = g_labels_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            verbose = 1, callbacks = [es])\n",
        "\n",
        "out = top_model.evaluate(G_test, g_labels_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6e4avqwJsG2"
      },
      "source": [
        "calc_gradient_val = 1\n",
        "if calc_gradient_val:\n",
        "  layer_names = [layer.name for layer in model.layers if 'conv' in layer.name]\n",
        "  cat_grads = [[[] for j in range(len(layer_names))] for i in range(ncats)]\n",
        "\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(ncats):\n",
        "    idx = list(np.where(labels == i))\n",
        "    cat_grads[i] = gc.calc_avg_gradcam(data_train[i],model,top_model)\n",
        "  print(f'Elapsed: {time.time() - start}s')\n",
        "\n",
        "  with open('gradient_values_cat', 'wb') as fp:\n",
        "      pickle.dump(cat_grads, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP_9oAIwLtgm"
      },
      "source": [
        "corr_all_2 = st.calc_all_corrcoeff(fc[0:2],cat_grads[0:2])\n",
        "corr_all_1 = st.calc_all_corrcoeff(fc[2:4],cat_grads[2:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i30AhQh_LmdR"
      },
      "source": [
        "import random\n",
        "\n",
        "cat_grad_s = cat_grads.copy()\n",
        "random.shuffle(cat_grad_s)\n",
        "\n",
        "for cat in range(len(cat_grad_s)):\n",
        "  for layer in range(len(cat_grad_s[cat])):\n",
        "    random.shuffle(cat_grad_s[cat][layer])\n",
        "\n",
        "corr_shuff = st.calc_all_corrcoeff(fc,cat_grad_s)\n",
        "pt.plot_corr(corr_all_1,corr_shuff,figsize = (20,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syf9wsRa6Fr2"
      },
      "source": [
        "# Neuron level activities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqmuwH9K5jpy"
      },
      "source": [
        "calc_neuron_activ = 1\n",
        "neuron_act = [[np.empty((224,224,64)),\n",
        "              np.empty((224,224,64)),\n",
        "              np.empty((112,112,128)),\n",
        "              np.empty((112,112,128)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((14,14,512)),\n",
        "              np.empty((14,14,512)),\n",
        "              np.empty((14,14,512))] for i in range(len(categories))]\n",
        "\n",
        "if calc_neuron_activ:\n",
        "  for cat in range(len(categories)): \n",
        "\n",
        "    print('Category of interest: ', categories[cat])\n",
        "    train_it = reg_train[cat]\n",
        "\n",
        "    layer_names = [layer.name for layer in model.layers if 'conv' in layer.name]\n",
        "    layer_outputs = [layer.output for layer in model.layers if layer.name in layer_names]\n",
        "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "    tun_activ = [[[np.empty((224,224,64)),\n",
        "              np.empty((224,224,64)),\n",
        "              np.empty((112,112,128)),\n",
        "              np.empty((112,112,128)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((56,56,256)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((28,28,512)),\n",
        "              np.empty((14,14,512)),\n",
        "              np.empty((14,14,512)),\n",
        "              np.empty((14,14,512))] for j in range(len(layer_names))] for i in range(len(train_it))]\n",
        "\n",
        "    start = time.time()\n",
        "    for imgs,img_tensor in enumerate(train_it):\n",
        "      img_tensor = img_tensor.reshape([1,224,224,3])\n",
        "      intermediate_activations = activation_model.predict(img_tensor)\n",
        "      for l in range(len(layer_names)):\n",
        "        layer_activation = intermediate_activations[l]\n",
        "        tun_activ[imgs][l] = layer_activation[0]\n",
        "    \n",
        "    neuron_act[cat] = np.mean(tun_activ,0)\n",
        "    print(f'Elapsed: {time.time() - start}s')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSZ0XyAL2v7W"
      },
      "source": [
        "x = np.array(neuron_act)\n",
        "for cat in range(len(categories)):\n",
        "  for layer in range(13):\n",
        "    l = len(x[cat,layer])\n",
        "    m = len(x[cat,layer][0,0])\n",
        "    x[cat,layer] = x[cat][layer].reshape(l*l*m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIBm4ySdRL3m"
      },
      "source": [
        "plt.figure()\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = x[0],palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,300])\n",
        "plt.title('Tuning quality for Male Faces')\n",
        "plt.figure()\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = x[1],palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,300])\n",
        "plt.title('Tuning quality for Female Faces')\n",
        "plt.figure()\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = x[2],palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,300])\n",
        "plt.title('Tuning quality for Manmade Scenes')\n",
        "plt.figure()\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data = x[3],palette='cool')\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Tuning Quality')\n",
        "ax.set_ylim([0,300])\n",
        "plt.title('Tuning quality for Natural Scenes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq7qo0z3JBA0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoqhbP8HixD0"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-etj3D0SVwfb"
      },
      "source": [
        "### Att\n",
        "(This is optional. The same code is written in vgg16obj/tools/model_calcs.py called separately in the next driver chunk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMxZPtSnWNdZ"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Oct 10 02:39:43 2020\n",
        "\n",
        "@author: soukhind\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.activations import relu\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from vis.utils import utils\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow import math\n",
        "import time\n",
        "from sklearn.metrics import roc_curve,accuracy_score,precision_recall_curve,f1_score\n",
        "\n",
        "def gen_attnmap(modifier,mask,category,bi,atype,rand_map):\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modifier : list\n",
        "        modifier to be used to implement attention.\n",
        "    mask : ndarray\n",
        "        binary vector to determine which layer to apply attention at. \n",
        "        include attention strength by multiplying to it\n",
        "    category : ndarray\n",
        "        cateogies .\n",
        "    bi : boolean \n",
        "        bidirectionality\n",
        "        True & False.\n",
        "    atype: int\n",
        "        1 = Multiplicative\n",
        "        2 = Additive\n",
        "    rand_map: bool, optional\n",
        "        Set to 'True' for random set of tuning values\n",
        "    Returns\n",
        "    -------\n",
        "    tensor_attnmap : tensor\n",
        "        attention map.\n",
        "\n",
        "    \"\"\"\n",
        "    attnmap = []\n",
        "    #beta = calc_beta(avg_tun_activ)/10\n",
        "  \n",
        "    #conv1_1 & conv1_2\n",
        "\n",
        "    for layer in range(2):\n",
        "        mapval = np.float32(modifier[category][layer])\n",
        "        if bi == False:\n",
        "            mapval[mapval < 0] = 0\n",
        "        if atype == 1:\n",
        "          amap = np.ones((224,224,64),dtype='float32') + np.tile(mapval,[224,224,1])* mask[layer]\n",
        "        elif atype == 2:\n",
        "          amap = np.tile(mapval,[224,224,1])* mask[layer]\n",
        "        #amap[amap < 0] = 0\n",
        "        if rand_map == 1:\n",
        "          random.shuffle(amap)\n",
        "        elif rand_map == 2:\n",
        "          \n",
        "          amap = np.ones((224,224,64),dtype='float32') +  mask[layer]\n",
        "        attnmap.append(amap)\n",
        "        \n",
        "    \n",
        "    #conv2_1 & conv2_2\n",
        "    for layer in range(2,4):\n",
        "        mapval = np.float32(modifier[category][layer])\n",
        "        if bi == False:\n",
        "            mapval[mapval < 0] = 0\n",
        "        if atype == 1:\n",
        "          amap = np.ones((112,112,128),dtype='float32') + np.tile(mapval,[112,112,1])* mask[layer]\n",
        "        elif atype == 2:\n",
        "          amap = np.tile(mapval,[112,112,1])* mask[layer]\n",
        "        #amap[amap < 0] = 0\n",
        "        if rand_map == 1:\n",
        "          random.shuffle(amap)\n",
        "        elif rand_map == 2:\n",
        "          amap = np.ones((112,112,128),dtype='float32') +  mask[layer]\n",
        "        attnmap.append(amap)\n",
        "    \n",
        "    #conv3_1 - conv3_3\n",
        "    for layer in range(4,7):\n",
        "        mapval = np.float32(modifier[category][layer])\n",
        "        if bi == False:\n",
        "            mapval[mapval < 0] = 0\n",
        "        if atype == 1:\n",
        "          amap = np.ones((56,56,256),dtype='float32') + np.tile(mapval,[56,56,1])* mask[layer]\n",
        "        elif atype == 2:\n",
        "          amap = np.tile(mapval,[56,56,1])* mask[layer]\n",
        "        #amap[amap < 0] = 0\n",
        "        if rand_map == 1:\n",
        "          random.shuffle(amap)\n",
        "        elif rand_map == 2:\n",
        "          amap = np.ones((56,56,256),dtype='float32') + mask[layer]\n",
        "        attnmap.append(amap)\n",
        "    \n",
        "    #conv4_1 - conv4_3\n",
        "    for layer in range(7,10):\n",
        "        mapval = np.float32(modifier[category][layer])\n",
        "        if bi == False:\n",
        "            mapval[mapval < 0] = 0\n",
        "        if atype == 1:\n",
        "          amap = np.ones((28,28,512),dtype='float32') + np.tile(mapval,[28,28,1])* mask[layer]\n",
        "        elif atype == 2:\n",
        "          amap = np.tile(mapval,[28,28,1])* mask[layer]\n",
        "        #amap[amap < 0] = 0\n",
        "        if rand_map == 1:\n",
        "          random.shuffle(amap)\n",
        "        elif rand_map == 2:\n",
        "          amap = np.ones((28,28,512),dtype='float32') + mask[layer]\n",
        "        attnmap.append(amap)\n",
        "    \n",
        "    #conv5_1 - conv5_3\n",
        "    for layer in range(10,13):\n",
        "        mapval = np.float32(modifier[category][layer])\n",
        "        if bi == False:\n",
        "            mapval[mapval < 0] = 0\n",
        "        if atype ==1:\n",
        "          amap = np.ones((14,14,512),dtype='float32') + np.tile(mapval,[14,14,1])* mask[layer]\n",
        "        elif atype == 2:\n",
        "          amap = np.tile(mapval,[14,14,1])* mask[layer]\n",
        "        #amap[amap < 0] = 0\n",
        "        if rand_map == 1:\n",
        "          random.shuffle(amap)\n",
        "        elif rand_map == 2:\n",
        "          amap = np.ones((14,14,512),dtype='float32') + mask[layer]\n",
        "        attnmap.append(amap)\n",
        "    \n",
        "    \n",
        "    \n",
        "    tensor_attnmap = []\n",
        "    for layer in range(len(attnmap)):\n",
        "      tensor_attnmap.append(tf.convert_to_tensor(attnmap[layer])) \n",
        "    \n",
        "    return tensor_attnmap\n",
        "\n",
        "\n",
        "\n",
        "def avg_accuracy(data_train,train_labels,\n",
        "                 data_test,test_labels,\n",
        "                 modifier,\n",
        "                 model,top_model,idxpath,\n",
        "                 category,\n",
        "                 atstrng,\n",
        "                 bidir = True,\n",
        "                 atype = 1,\n",
        "                 rand_map = 0):\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_train : ndarray\n",
        "        Training data.\n",
        "    train_labels : categorical\n",
        "        Training labels.\n",
        "    data_test : ndarray\n",
        "        Testing data.\n",
        "    test_labels : categorical\n",
        "        Testing labels.\n",
        "    categories : ndarray\n",
        "        Names of each category.\n",
        "    modifier : list\n",
        "        modifier to be used to implement attention.\n",
        "    model : keras model\n",
        "        base model.\n",
        "    top_model : keras model\n",
        "        top model.\n",
        "    idxpath : string\n",
        "        for internal use.\n",
        "    atstrng : float32\n",
        "        attention strength.\n",
        "    bidir : bool, optional\n",
        "        Bidirectionality. The default is True.\n",
        "    atype: int\n",
        "        1 = Multiplicative\n",
        "        2 = Additive\n",
        "    rand_map: int, optional\n",
        "        0 = No randomization\n",
        "        1 = Shuffled tuning values\n",
        "        2 = Same tuning values\n",
        "    Returns\n",
        "    -------\n",
        "    t_acc\n",
        "        Accuracy for each category at each layer.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    epochs = 30    \n",
        "    n_layers = 13\n",
        "    t_acc = np.zeros(n_layers)\n",
        "    thr = np.zeros(n_layers)\n",
        "    for li in range(n_layers):\n",
        "        layermask = np.zeros(13)\n",
        "        layermask[li] = 1\n",
        "        tensor_attnmap = gen_attnmap(modifier,layermask*atstrng,category,bidir,atype,rand_map)     \n",
        "        def attnrelu(x,map = tensor_attnmap,atype = atype):\n",
        "            layeridx = np.load(idxpath)\n",
        "            if layeridx == 13:\n",
        "                layeridx = 0\n",
        "            if atype == 1:\n",
        "              x = nn.relu(x)\n",
        "              activations = math.multiply(x,map[layeridx])\n",
        "            if atype == 2:\n",
        "              activations = math.add(x,map[layeridx])\n",
        "              activations = nn.relu(activations)\n",
        "            layeridx += 1\n",
        "            np.save(idxpath,layeridx)\n",
        "            return activations\n",
        "    \n",
        "        get_custom_objects().update({'attnrelu': Activation(attnrelu)})\n",
        "    \n",
        "        for layer in model.layers:\n",
        "            if(hasattr(layer,'activation')):\n",
        "                layer.activation = attnrelu\n",
        "    \n",
        "        utils.apply_modifications(model)\n",
        "        model.compile()\n",
        "        \n",
        "        f_train = model.predict(data_train)     \n",
        "    \n",
        "        f_test = model.predict(data_test)\n",
        "        es = EarlyStopping(monitor='loss', mode='min', verbose=1)\n",
        "  \n",
        "        history = top_model.fit(x = f_train,  y = train_labels,\n",
        "                epochs=epochs,\n",
        "                batch_size=64,\n",
        "                verbose = 0, callbacks = [])\n",
        "    \n",
        "        out = top_model.evaluate(f_test, test_labels)\n",
        "        t_acc[li] = out[1]\n",
        "\n",
        "    return t_acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP0gLmlmVz7q"
      },
      "source": [
        "### Driver\n",
        "This is the big loop which applies attention at each layer separately as a function of different attentional strengths. \n",
        "PS. This is very heavy and requires hours to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0foPTv5Git5q"
      },
      "source": [
        "#@title Multiplicative and bidirectionality corrected\n",
        "\n",
        "from vgg16obj.tools import model_calcs as mc\n",
        "import gc\n",
        "layeridx = 0\n",
        "np.save('layeridx',layeridx)\n",
        "#atstrng = 10.5 #0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5,8,8.5,9,9.5,10\n",
        "for atstrng in np.arange(0,10,0.5): #Calling for different attn strengths\n",
        "  grand_acc = [[] for i in range(ncats)] \n",
        "  for cat in range(ncats): # Iterating over 6 categories\n",
        "    tf.keras.backend.clear_session()\n",
        "    print('Category of interest: ', categories[cat])\n",
        "    train_it = np.concatenate((reg_train[cat],reg_train[cat + 4])) # Training on regular data\n",
        "    test_it = np.concatenate((data_test[cat],data_test[cat + 4])) # Testing on merged data\n",
        "    print(train_it.shape,test_it.shape)\n",
        "    model = VGG16(weights='imagenet',\n",
        "                        include_top=False,input_shape = [224,224,3])\n",
        "          \n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=features_train.shape[1:])) \n",
        "    top_model.add(Dense(4096, activation='relu',name = 'top_dense1')) \n",
        "    top_model.add(Dense(4096, activation='relu',name = 'top_dense2')) \n",
        "\n",
        "    top_model.add(Dense(2, activation='softmax',name = 'predictions'))\n",
        "    top_model.compile(optimizer= Adam(lr=1e-5),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    # Read avg_accuracy's definition to learn about the parameters\n",
        "    grand_acc[cat] = avg_accuracy(train_it,\n",
        "                              train_labels,\n",
        "                              test_it,\n",
        "                              test_labels,\n",
        "                              fc,\n",
        "                              model,\n",
        "                              top_model,\n",
        "                              '/content/layeridx.npy',\n",
        "                              cat,\n",
        "                              atstrng,\n",
        "                              bidir = True,\n",
        "                              atype = 1,\n",
        "                              rand_map = 2 )\n",
        "  grand_acc = np.array(grand_acc)\n",
        "  grand_acc.reshape([13,4,1])\n",
        "  # Use tun_accuracy_multi for multiplicative mode (atype = 1) and\n",
        "  # use tun_accuracy_bias for bias mode (atype = 2)\n",
        "  np.save('tun_accuracy_multi' + str(atstrng),grand_acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D_xfrLyptgA"
      },
      "source": [
        "grand_acc = np.array(grand_acc)\n",
        "grand_acc.reshape([1,6,13])\n",
        "np.save('tun_accuracy_new' + str(atstrng),grand_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aeOcsA8jVFs"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"white\",rc={\"lines.linewidth\": 0.7})\n",
        "plt.figure(figsize = (15,5))\n",
        "ax = sns.pointplot(data = grand_acc, color = 'mediumseagreen',\n",
        "                   errorwidth = 0.1 , capsize = 0.2)\n",
        "ax.set_xlabel('Layer',size = 20)\n",
        "ax.set_xticklabels(np.arange(1,14))\n",
        "ax.set_ylabel('Avg. acc.',size = 20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8oDGHaSuqf"
      },
      "source": [
        "### Regular Accuracy\n",
        "Calculate the regular accuracy so that they can be contrasted against when attention is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLuKzW56SuEZ"
      },
      "source": [
        "# Reinitialising the models for calculating regular accuracy\n",
        "model = VGG16(weights='imagenet',\n",
        "                        include_top=False,input_shape = [224,224,3])\n",
        "          \n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=features_train.shape[1:])) \n",
        "top_model.add(Dense(4096, activation='relu',name = 'top_dense1')) \n",
        "top_model.add(Dense(1, activation='sigmoid',name = 'predictions'))\n",
        "top_model.compile(optimizer= Adam(lr=1e-5),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "    \n",
        "categories = ['Male','Female','Manmade','Natural']\n",
        "acc = [0]*4\n",
        "for cat in range(ncats):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  print('Category of interest: ', categories[cat])\n",
        "  train_it = np.concatenate((reg_train[cat],reg_train[cat + 4]))\n",
        "  test_it = np.concatenate((data_test[cat],data_test[cat + 4]))\n",
        "  print(train_it.shape,test_it.shape)\n",
        "\n",
        "\n",
        "  #plot_model(model,show_shapes=True,expand_nested=True)\n",
        "  #model.save_weights('vgg_w',save_format='h5')\n",
        "\n",
        "  features_train = model.predict(train_it) \n",
        "\n",
        "  features_test = model.predict(test_it) \n",
        "\n",
        "\n",
        "  epochs = 30\n",
        "  #train_data = np.load('features_train.npy')\n",
        "  ntrain = 80\n",
        "  train_data = features_train\n",
        "  train_labels = np.array([0] * ntrain + [1]*ntrain)\n",
        "\n",
        "\n",
        "  #test_data = np.load('features_test.npy')\n",
        "  ntest = 40\n",
        "  test_data = features_test\n",
        "  test_labels = np.array([0] * ntest + [1]*ntest)\n",
        "\n",
        "\n",
        "\n",
        "  es = EarlyStopping(monitor='loss', mode='min', verbose=1)\n",
        "\n",
        "  history = top_model.fit(x = train_data,  y = train_labels,\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            verbose = 0, callbacks = [es])\n",
        "\n",
        "  out = top_model.evaluate(test_data, test_labels)\n",
        "  print(out)\n",
        "  acc[cat] = out[1]\n",
        "  #print('rounded test labels',pred)\n",
        "np.mean(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOOHYrOnLHRk"
      },
      "source": [
        "# Calculation for control condition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGj72j787vQd"
      },
      "source": [
        "plt_data = np.array((3,3,0,0.8))\n",
        "fig = plt.figure(figsize = (8,4))\n",
        "plt.bar(x = ['Male','Female','Manmade','Natural'],\n",
        "        height = plt_data.T,\n",
        "        color = 'mediumseagreen')\n",
        "plt.title('Control Condition')\n",
        "plt.ylabel('Increase in performance (%)')\n",
        "plt.xlabel('Category')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrX-2ukIQ1wV"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Pl1kB3Q3vO"
      },
      "source": [
        "### Layer wise plot\n",
        "This code generates the plot for performance increase as a function of attention applied at each layer separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMr8HqKU6gb"
      },
      "source": [
        "acc = [0.69,0.72,0.85,0.58]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.load('tun_accuracy_multi0.npy')\n",
        "x = np.expand_dims(x,0)\n",
        "\n",
        "for i in np.arange(1,10,1):\n",
        "  temp = np.load('tun_accuracy_multi' + str(i) + '.npy')\n",
        "  temp = np.expand_dims(temp,0)\n",
        "  x = np.vstack((x,temp))\n",
        "\n",
        "\n",
        "\n",
        "x = np.amax(x,0)\n",
        "\n",
        "plt_data = np.zeros((ncats,13))\n",
        "for i in range(ncats):\n",
        "  for j in range(13):\n",
        "    plt_data[i,j] = x[i,j] - acc[i]\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"white\",rc={\"lines.linewidth\": 1})\n",
        "plt.figure(figsize = (15,12))\n",
        "ax = sns.pointplot(data = plt_data*100, color = 'mediumseagreen',\n",
        "                   errorwidth = 0.1 , capsize = 0.2)\n",
        "                   \n",
        "ax.set_xlabel('Layer',size = 20)\n",
        "ax.set_xticklabels(np.arange(1,14),size = 20)\n",
        "plt.yticks(fontsize = 20)\n",
        "plt.ylim(0,30)\n",
        "plt.title('Attention by tuning curve, bias stng: 0 - 10.5',size = 15)\n",
        "ax.set_ylabel('Performance Increase (%)',size = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xvHyvKjmefC"
      },
      "source": [
        "# NOT REQUIRED FOR NOW\n",
        "#grad_acc = plt_data\n",
        "#tun_acc = plt_data\n",
        "\n",
        "from scipy import stats\n",
        "pval = np.zeros(13)\n",
        "\n",
        "for i in range(13):\n",
        "  _,pval[i] = stats.ttest_ind(tun_acc[:,i],grad_acc[:,i],equal_var=False)\n",
        "\n",
        "pval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C8vZVeCQ9Dj"
      },
      "source": [
        "### Category wise plot\n",
        "This code generates the plot for performance increase as a function of category and attention applied at each layer separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVAZyDCmQ7sE"
      },
      "source": [
        "acc = [0.75,0.72,0.68,0.58]\n",
        "\n",
        "gacc = np.load('tun_accuracy_multi0.npy')\n",
        "gacc = np.expand_dims(gacc,0)\n",
        "\n",
        "for i in np.arange(1,10,1):\n",
        "  temp = np.load('tun_accuracy_multi' + str(i) + '.npy')\n",
        "  temp = np.expand_dims(temp,0)\n",
        "  gacc = np.vstack((gacc,temp))\n",
        "strngs = np.arange(0.5,5,1)\n",
        "\n",
        "for i in np.arange(1.5,10,0.5):\n",
        "  temp = np.load('tun_accuracy_multi' + str(i) + '.npy')\n",
        "  temp = np.expand_dims(temp,0)\n",
        "  gacc = np.vstack((gacc,temp))\n",
        "strngs = np.concatenate((strngs,np.arange(1,6,1)))\n",
        "y = np.amax(gacc,0).T\n",
        "\n",
        "\n",
        "plt_data = np.zeros((13,4))\n",
        "for i in range(13):\n",
        "  for j in range(4):\n",
        "    plt_data[i,j] = y[i,j] - acc[j]\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\",rc={\"lines.linewidth\": 1})\n",
        "sns.set(font_scale=2)\n",
        "layer = 0\n",
        "f,axes = plt.subplots(2,2,figsize=(15,15))\n",
        "f.add_subplot(111, frameon=False)\n",
        "# hide tick and tick label of the big axis\n",
        "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
        "plt.xlabel(\"Layer\",size = 15)\n",
        "plt.ylabel(\"Performance Increase (%)\",size = 15)\n",
        "for x in range(2):\n",
        "  for y in range(2):\n",
        "    axes[x,y].bar(x = np.arange(0,13),height = plt_data[:,layer]*100,color = 'mediumseagreen')\n",
        "    axes[x,y].set_xticklabels(np.arange(1,14),size = 15)\n",
        "    axes[x,y].set_xticks(np.arange(0,14))\n",
        "    axes[x,y].set_title(categories[layer],size = 15)\n",
        "    axes[x,y].set_ylim(-2,35)\n",
        "    axes[x,y].set_xlim(-1,13)\n",
        "    axes[x,y].hlines(y = 0,xmin= -1,xmax = 13)\n",
        "    layer+=1\n",
        "\n",
        "np.save('merge_set.npy',plt_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}